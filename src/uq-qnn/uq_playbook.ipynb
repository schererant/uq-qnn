{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import strawberryfields as sf\n",
    "from strawberryfields.ops import *\n",
    "import tensorflow as tf\n",
    "import random as rd\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input\n",
    "from datetime import datetime\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.special import comb\n",
    "from tensorflow.keras.initializers import RandomUniform, Identity\n",
    "from tensorflow.keras.layers import Dense, ReLU\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, TensorBoard\n",
    "import pickle\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the interpretation of this function and how does one get to it?\n",
    "# (A): x aktuell, y1/y2 vergangenheit, memerisot change function?\n",
    "def function_lag(x,y1,y2):\n",
    "    return 0.4*y1+0.4*y1*y2+0.6*x**3+0.1\n",
    "\n",
    "# what is the interpretation of this function - could one name it accordingly? Like multiply_3inputs?\n",
    "\n",
    "# prediction fnction xt, xt-1, xt-2\n",
    "def function_lag_iris(x1,x2,x3):\n",
    "    return x1*x2*x3\n",
    "\n",
    "def target_function(xt, xt1, 2):\n",
    "    return 0.4*xt1+0.4*xt1*xt2+0.6*xt**3+0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=np.random.random_sample(100)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Memristor time lag\")\n",
    "res_mem={}\n",
    "prove_num=1 #?\n",
    "\n",
    "# Linear square und cube functions to predict\n",
    "for power in [3]:\n",
    "    dip=power # memory? why call it dip? \n",
    "    res_mem={}\n",
    "\n",
    "    for p in range(prove_num):\n",
    "        eng = sf.Engine(backend=\"tf\", backend_options={\"cutoff_dim\": 4})\n",
    "        \n",
    "        # create 3 mode circuit\n",
    "        circuit=sf.Program(3)\n",
    "\n",
    "        # use random training points\n",
    "        x_train=np.sqrt(inp)\n",
    "        \n",
    "        # translate values to angles -> encoding\n",
    "        angs=2*np.arccos(x_train)\n",
    "    \n",
    "    \n",
    "        \n",
    "        print(\"prova %d su %d\" %(p, prove_num)) # english?\n",
    "\n",
    "        # first memoristor phase (random)\n",
    "        phi1 = tf.Variable(rd.uniform(0.01,1)*2*np.pi, constraint=lambda z: tf.clip_by_value(z, 0, 2*np.pi))\n",
    "\n",
    "        # what for? phi 2?\n",
    "        x_2=tf.Variable(rd.uniform(0.01,1),  constraint=lambda z: tf.clip_by_value(z, 0.01, 1)) # free parameter in phi2, phi2 is memoristor \n",
    "        p1=tf.constant(rd.uniform(0.01,1))\n",
    "        p2=tf.Variable(rd.uniform(0.01,1))\n",
    "\n",
    "        # input training data\n",
    "        phienc = tf.constant(angs)\n",
    "\n",
    "         # second memoristor phase (random)\n",
    "        phi3 = tf.Variable(rd.uniform(0.01,1)*2*np.pi, constraint=lambda z: tf.clip_by_value(z, 0, 2*np.pi))\n",
    "        print(\"Random choices: \", phi1, phi3, x_2)\n",
    "        \n",
    "        phi_1, phi_2,phi_3, phi_enc = circuit.params(\"phi1\",\"phi2\", \"phi3\", \"phienc\")\n",
    "        \n",
    "        \n",
    "        # for UQ stuff we want to have the circuit below with the memristor parametrized by phi_2 and then we want to make the parameters\n",
    "        # for the MZIs phi_1 and phi_3 stochastic\n",
    "        # that means sample phi_1_sample in np.normal(phi_1, var)  and phi_3_sample in np.normal(phi_3, var)\n",
    "        # where we experiment with the var>0 and phi_1 and phi_3 are obtained from the training loop below   \n",
    "            \n",
    "        with circuit.context as q:\n",
    "            Vac     | q[0]\n",
    "            Fock(1) | q[1]\n",
    "            Vac     | q[2]\n",
    "        \n",
    "            ###----------------------------------------------------------------------\n",
    "        \n",
    "            # input \n",
    "            ## encoding MZI\n",
    "        \n",
    "            BSgate(np.pi/4, np.pi/2) | (q[0], q[1])\n",
    "        \n",
    "            Rgate(phi_enc)           | q[1] # encoding\n",
    "        \n",
    "            BSgate(np.pi/4, np.pi/2) | (q[0], q[1])\n",
    "            \n",
    "            ## first MZI\n",
    "        \n",
    "            BSgate(np.pi/4, np.pi/2) | (q[0], q[1])\n",
    "        \n",
    "            Rgate(phi_1)              | q[1] # state preparation\n",
    "        \n",
    "            BSgate(np.pi/4, np.pi/2) | (q[0], q[1])\n",
    "            \n",
    "             ## second MZI\n",
    "        \n",
    "            BSgate(np.pi/4, np.pi/2) | (q[1], q[2])\n",
    "        \n",
    "            Rgate(phi_2)              | q[1] # memoristor value\n",
    "        \n",
    "            BSgate(np.pi/4, np.pi/2) | (q[1], q[2])\n",
    "            \n",
    "            \n",
    "            ## third MZI\n",
    "        \n",
    "            BSgate(np.pi/4, np.pi/2) | (q[0], q[1])\n",
    "        \n",
    "            Rgate(phi_3)              | q[1] # state tomography\n",
    "        \n",
    "            BSgate(np.pi/4, np.pi/2) | (q[0], q[1])\n",
    "            \n",
    "            \n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.003)\n",
    "        # we should also do an ablation study on the number of steps for training\n",
    "        # in the error plot it looked like at least 50 steps is ok-ish\n",
    "        steps = 1\n",
    "        \n",
    "        for step in range(steps):\n",
    "        \n",
    "            # reset the engine if it has already been executed\n",
    "            if eng.run_progs:\n",
    "                eng.reset()\n",
    "        \n",
    "            with tf.GradientTape() as tape:\n",
    "                loss=0\n",
    "                index=0\n",
    "                p1=np.zeros(dip) # dip ist memory\n",
    "                p2=np.zeros(dip)\n",
    "                for phi in range(len(phienc)): # 0 - all datapoints\n",
    "                    t=phi-index*dip # index 0 - 3 // t 0-2\n",
    "                    if t==dip-1: \n",
    "                        index+=1\n",
    "                    if phi==0: # if memory empty, start with sq 0.5 \n",
    "                        results = eng.run(circuit, args={\"phi1\": phi1, \"phi3\": phi3, \"phi2\": tf.Variable(tf.acos(np.sqrt(0.5))), \"phienc\": phienc[phi]})\n",
    "                    if phi>0: # if not empty use last values until max dip --> memoristor juhu\n",
    "                        results = eng.run(circuit, args={\"phi1\": phi1, \"phi3\": phi3, \"phi2\": tf.acos(tf.sqrt(np.sum(p1)/dip+x_2*np.sum(p2)/dip)), \"phienc\": phienc[phi]})\n",
    "\n",
    "                    # get probabilities\n",
    "                    #in the UQ version we get the prob per sample of phi_1 and phi_3, meaning that for n samples we get \n",
    "                    # n prob results, but not in the training loop and initially only in the predict loop\n",
    "                    prob=results.state.all_fock_probs()\n",
    "\n",
    "                    # then we compute the mean and variance (or std) over samples\n",
    "                    # so for each t we can either store two values the mean prediction and the variance (or std) or just straight\n",
    "                    # up n samples and then later do the computation on these\n",
    "\n",
    "                    # store prob in memory p1 (index at t)\n",
    "                    p1[t]=tf.Variable(np.real(prob[0,1,0])) # store prob in memory p1 (index at t)\n",
    "                    p2[t]=tf.Variable(np.real(prob[0,0,1]))\n",
    "        \n",
    "                    if phi>=2: # should be phi > dip\n",
    "                        # why is the to the power 6 part left out here? because there is no memristor?\n",
    "                        f2=function_lag_iris(x_train[phi]**2,x_train[phi-1]**2,x_train[phi-2]**2) # function to predict, current, past, past\n",
    "                        \n",
    "                        # what does the to the power 6 version do? Ask Iris?\n",
    "                        #f1=x_train[phi]**6\n",
    "                        #loss += (abs(f2-prob[0,1,0])+abs(f1-prob[0,0,1]))**2\n",
    "                        loss += (abs(f2-prob[0,0,1]))**2\n",
    "\n",
    "            gradients = tape.gradient(loss, [phi1, phi3, x_2]) # get gradients\n",
    "            opt.apply_gradients(zip(gradients, [phi1, phi3, x_2]))\n",
    "            res_mem[('loss', 'tr', step, p, power)]=[loss, phi1.value(), phi3.value(), x_2.value(), inp] # store results of memoristor for log\n",
    "            print(\"Loss at step {}: {}\".format(step+1, np.real(loss)))\n",
    "        print(\"Final loss: %f\" %np.real(loss))\n",
    "        print(\"Optimal parameters: %f and %f\\n x2= %f\" %(phi1,phi3, x_2))\n",
    "    file=open(\"results_mem_\"+str(power)+\"_t_lag_iris_lr_lag_try\", \"wb\")\n",
    "    pickle.dump(res_mem, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here then we can compare no memristor with sampled weights vs no memristor without sampled weights\n",
    "# and the same with a memristor\n",
    "\n",
    "\n",
    "# ablation study if the std or variance is useful for setting the parameter (var in the normal dist for phi_1 and phi_3)\n",
    "# compute the quantiles over the std for all inputs sampled in (0,100) \n",
    "# then if in prediction mode for some x in (0,100) the std >0.8 (e.g 0.8 but let's try all quantiles)\n",
    "# omit that point x from the RMSE computation\n",
    "\n",
    "for p in ['3']:\n",
    "    #file=open(\"function prediction/powers/results_mem_\"+p+\"_power_2\", \"rb\")\n",
    "    file=open(\"results_mem_3_t_lag_iris_lr_003_274\", \"rb\")\n",
    "    # can one write out r4m and r4nm - memristor and no memristor? reservoir 4?\n",
    "    r4m=pickle.load(file)\n",
    "    file.close()\n",
    "    file=open(\"results_nomem_3_t_lag_iris_lr_003_274\", \"rb\")\n",
    "    r4nm=pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    loss_memory=[]\n",
    "    for x in range(10):\n",
    "        loss_memory.append(r4m[('loss', 'tr',59,x, int(p))][0])\n",
    "        \n",
    "    index_m=np.argmin(loss_memory)\n",
    "    #index_m=2\n",
    "    \n",
    "    loss=[]\n",
    "    for x in range(10):\n",
    "        loss.append(r4nm[('loss', 'tr', 59,x, int(p))][0])\n",
    "        \n",
    "    index_nomem=np.argmin(loss)\n",
    "    r4nm[('loss', 'tr', 59,index_nomem, int(p))]\n",
    "    \n",
    "    r4_nm=[]\n",
    "    r4_m=[]\n",
    "    for i in range(60):\n",
    "        r4_nm.append(r4nm[('loss', 'tr', i,index_nomem, int(p))][0])   \n",
    "        r4_m.append(r4m[('loss', 'tr', i,index_m, int(p))][0])   \n",
    "    sp=str(p)    \n",
    "    plt.plot(np.real(np.array(r4_nm)), label=r'No memristor $f(x)=x^'+sp+'$')\n",
    "    plt.plot(np.real(np.array(r4_m)), label=r'Memristor $f(x)=x^'+sp+'$')\n",
    "    #plt.yticks(np.array([0,2,4,6]), fontsize=18)\n",
    "    plt.xticks(np.array([0,10,20,30,40,50, 60]), fontsize=18)\n",
    "    plt.xlabel(\"step\", fontsize=23)\n",
    "    plt.ylabel(\"loss\", fontsize=23)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.tight_layout\n",
    "print(loss_memory[index_m])\n",
    "o_p=r4m['loss', 'tr',59,index_m, 3]\n",
    "\n",
    "# is this loss \"good\" in any sense? Because if the interval is [0,1], anything above 10 in RMSE is shit or? or is the x range:  x in range(10)?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
